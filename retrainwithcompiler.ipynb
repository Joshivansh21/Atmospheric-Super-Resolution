{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7062992,"sourceType":"datasetVersion","datasetId":4066396},{"sourceId":7063018,"sourceType":"datasetVersion","datasetId":4066417},{"sourceId":7087718,"sourceType":"datasetVersion","datasetId":4083832},{"sourceId":7088295,"sourceType":"datasetVersion","datasetId":4084230}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nstrategy = tf.distribute.MirroredStrategy()\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy import expand_dims\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy.random import randn\nfrom numpy.random import randint\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Reshape\nfrom keras.layers import Flatten\nfrom keras.layers import Conv2D\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Add\nfrom keras.models import load_model\nfrom keras.layers import AveragePooling2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import Concatenate\nfrom keras.layers import GaussianNoise\nfrom keras.layers import BatchNormalization\nfrom keras.layers import LayerNormalization\nfrom keras.layers import Conv3D\nfrom keras.layers import ConvLSTM3D\nfrom keras.layers import ConvLSTM2D\nfrom keras.layers import TimeDistributed\nfrom keras.initializers import RandomNormal\nimport keras.backend as K\nfrom sklearn.utils import shuffle\n\nclass WGAN(keras.Model):\n    def __init__(self, discriminator, generator, Dsteps=5, gp_weight=10.0):\n        super(WGAN, self).__init__()\n\n        self.discriminator = discriminator\n        self.generator = generator\n        self.d_steps = Dsteps\n        self.gp_weight = gp_weight\n\n    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n        super(WGAN, self).compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.d_loss_fn = d_loss_fn\n        self.g_loss_fn = g_loss_fn\n    \n    \n    def gradient_penalty (self,batch_size, real_images, fake_images):\n        alpha = tf.random.uniform([batch_size, 1, 1, 1], minval=0.,maxval=1.)\n        diff = fake_images - real_images\n        interpolated = real_images + alpha * diff\n\n        with tf.GradientTape() as gp_tape:\n            gp_tape.watch(interpolated)\n            pred = self.discriminator(interpolated, training=True)\n\n        grads = gp_tape.gradient(pred, [interpolated])[0]\n        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n        gp = tf.reduce_mean((norm - 1.0) ** 2)\n        return gp                             \n\n    def train_step(self, data):\n        #if isinstance(data, list):\n        lowres_images = data[0]\n        real_images=data[1]\n        batch_size = tf.shape(real_images)[0]\n\n        for i in range(self.d_steps):\n            with tf.GradientTape() as tape:\n                # Generate fake images from the latent vector\n                fake_images = self.generator(lowres_images, training=True)\n                # Get the logits for the fake images\n                fake_logits = self.discriminator(fake_images, training=True)\n                # Get the logits for the real images\n                real_logits = self.discriminator(real_images, training=True)\n                # Calculate the discriminator loss using the fake and real image logits\n                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n                # Calculate the gradient penalty\n                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n                # Add the gradient penalty to the original discriminator loss\n                d_loss = d_cost + gp * self.gp_weight\n                # Get the gradients w.r.t the discriminator loss\n                d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n                # Update the weights of the discriminator using the discriminator optimizer\n                self.d_optimizer.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n        # Train the generator\n        #lowres_images= self.generate_low_res_samples(real_images)\n        with tf.GradientTape() as tape:\n            # Generate fake images using the generator\n            generated_images = self.generator(lowres_images, training=True)\n            # Get the discriminator logits for fake images\n            gen_img_logits = self.discriminator(generated_images, training=True)\n            # Calculate the generator loss\n            g_loss = self.g_loss_fn(gen_img_logits)\n        # Get the gradients w.r.t the generator loss\n        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n        # Update the weights of the generator using the generator optimizer\n        self.g_optimizer.apply_gradients(zip(gen_gradient, self.generator.trainable_variables))\n        return {\"d_loss\": d_loss, \"g_loss\": g_loss, 'batch_size':batch_size}\n\n# define a callback to save keras model after every epoch      \nclass GANMonitor(keras.callbacks.Callback):\n    def __init__(self, gen_loss, critic_loss):\n        self.gen_loss=gen_loss\n        self.critic_loss=critic_loss\n    \n    def on_epoch_end(self, epoch, logs={}):\n        g_model.save('TempSRgencomb%.1f.keras'%epoch)\n        d_model.save('TempSRdisc%.1f.keras'%epoch)\n        #pretraining.save('TempSRpretrain%.1f.keras'%epoch)\n       # model.save('./MyModel_tf',save_format='tf')\n        self.gen_loss.append(logs.get('g_loss'))\n        self.critic_loss.append(logs.get('d_loss'))\n\n# list to track the losses for training the WGAN-GP   \ngen_loss=[]\ncritic_loss=[]\n\n# tensorflow method to call all available gpu's for training.\nwith strategy.scope():\n    #define the optimizer for generator and discriminator\n    #pretraining_optimizer= keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.2, beta_2=0.9)\n    generator_optimizer = keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.2, beta_2=0.9)\n    discriminator_optimizer = keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.2, beta_2=0.9)\n    \n    def MSE_weighted(y_true,y_pred):\n        return K.mean(tf.multiply(tf.square(y_true),tf.square(tf.subtract(y_pred, y_true))))\n\n    #define the pretraining loss function\n    def corrector_loss(y_true, y_pred):\n        #calculating the soft discritization FSS score with coutoff 0.5 on images scaled [0,1]\n        gamma=0.1\n        c=10\n        cutoff=0.5\n        eps = K.epsilon()\n        y_true_bi = tf.math.sigmoid( c * ( y_true - cutoff ))\n        y_pred_bi = tf.math.sigmoid( c * ( y_pred - cutoff ))\n        MSE_n = MSE_weighted(y_true_bi,y_pred_bi) \n        #tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)(y_true_bi, y_pred_bi)\n        O_sqimg = tf.keras.layers.Multiply()([y_true_bi, y_true_bi])   \n        O_sqvec = tf.keras.layers.Flatten()(O_sqimg)\n        M_sqimg = tf.keras.layers.Multiply()([y_pred_bi, y_pred_bi])\n        M_sqvec = tf.keras.layers.Flatten()(M_sqimg)\n        MSE_ref = tf.math.reduce_mean(O_sqvec + M_sqvec)\n        return (tf.math.reduce_mean(tf.keras.losses.huber(y_true, y_pred, delta=0.1)+ gamma*(float(MSE_n) / float(MSE_ref+eps))))\n    \n                \n    #critic loss without the gradient penalty ter\n    def discriminator_loss(real_img, fake_img):\n        real_loss = tf.reduce_mean(real_img)\n        fake_loss = tf.reduce_mean(fake_img)\n        return fake_loss - real_loss\n    \n    #generator loss \n    def generator_loss(fake_img):\n        fake_loss= -tf.reduce_mean(fake_img)\n        return fake_loss\n    \n    #loading the model\n    d_model = load_model(\"/kaggle/input/models12epoch/TempSRdisc11.0.keras\")\n    g_model= load_model(\"/kaggle/input/models12epoch/TempSRgencomb11.0.keras\")\n    #pretraining= load_model(\"/kaggle/input/models12epoch/TempSRpretrain11.0.keras\", compile=False)\n    cbk = GANMonitor(gen_loss, critic_loss)\n    \n    #compile the pretraining model using low_res_gen\n    #pretraining.compile(optimizer=pretraining_optimizer, loss= corrector_loss)        \n    #define the model with generator and critic\n    wgan = WGAN(discriminator=d_model, generator=g_model, Dsteps=5)\n    # Compile the WGAN model.\n    wgan.compile(d_optimizer=discriminator_optimizer, g_optimizer=generator_optimizer, g_loss_fn=generator_loss, d_loss_fn=discriminator_loss,)       \n    \n#create a low resolution of the observational data to feed the pre-training   \ndef Pooling(High_Res_Data):\n    Avgpool= MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')\n    low1=Avgpool(High_Res_Data)\n    low2=Avgpool(low1)\n    return low2\n\n#load the data\nInput_data=np.load(\"/kaggle/input/retrainingimdera/Inputfiletemp.npy\")\nHighres_data=np.load(\"/kaggle/input/retrainingimdera/Highresfiletemp.npy\")\nLow_Res_Data, High_Res_Data= shuffle(Input_data, Highres_data)\nprint(Low_Res_Data.shape,High_Res_Data.shape)\n\n#define the batch size and epochs\npre_epoch=10\npre_batch_size=128\ntraining_epoch =20\ntraining_batch_size=128\ntotal_samples=High_Res_Data.shape[0]\n\n#maintain consistent number of samples per epoch\nstpe_wgan=total_samples//training_batch_size\nstpe_pretrain=total_samples//pre_batch_size\n#create a low resolution of the observational data set\nMaxpooled_data=Pooling(High_Res_Data)\n\n#Train the model.\n#pretraining.fit(Low_Res_Data, Maxpooled_data,batch_size=pre_batch_size,epochs=pre_epoch,steps_per_epoch=stpe_pretrain, verbose=2)\nwgan.fit(Low_Res_Data, High_Res_Data, batch_size=training_batch_size, epochs=training_epoch,callbacks=[cbk],steps_per_epoch=stpe_wgan, verbose=2)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T10:01:08.822540Z","iopub.execute_input":"2023-11-27T10:01:08.823016Z","iopub.status.idle":"2023-11-27T11:02:09.831591Z","shell.execute_reply.started":"2023-11-27T10:01:08.822975Z","shell.execute_reply":"2023-11-27T11:02:09.830073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom numpy import expand_dims\nimport numpy.ma as ma\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport matplotlib.cm as mtpltcm\nfrom mpl_toolkits.mplot3d import Axes3D\nimport random\nfrom netCDF4 import Dataset\nimport cartopy.crs as ccrs\nfrom tensorflow import keras\nfrom keras.layers import AveragePooling2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Add\nfrom keras.models import load_model\nimport numpy.ma as ma\nimport os\nimport glob\nnp.set_printoptions(threshold=np.inf)\n\nINPUT=np.load(\"/kaggle/input/training-error/Inputfiletemp2017-2019.npy\")\nOUTPUT=np.load(\"/kaggle/input/training-error/Highresfiletemp2017-2019.npy\")\nfiles=glob.glob(os.path.join(\"/kaggle/input/gen-epochs\", \"*.keras\"))\nmodels = [load_model(file) for file in files]\nRMSE=tf.keras.metrics.RootMeanSquaredError()\nprint(INPUT.shape, OUTPUT.shape) \nRMSEMean=[]\nRMSESD=[]\nfor j in range(len(models)):\n    print(\"model\",j,\"starting\")\n    Error=[]\n    for i in range(INPUT.shape[0]):\n        gen=INPUT[i]\n        era=OUTPUT[i]\n        era=era*51\n        era=np.reshape(era, (48,48))\n        gen=np.reshape(gen, (1, 12, 12, 1))\n        OUT = models[j].predict(gen, verbose=0)\n        OUT=np.reshape(OUT,(48,48))\n        OUT=OUT*51\n        Error.append(RMSE(OUT,era))\n    RMSEMean.append(np.mean(np.array(Error)))\n    RMSESD.append(np.std(np.array(Error)))\n\nnp.save(\"Trainerrormean\", np.array(RMSEMean))\nnp.save(\"Trainerrorsd\", np.array(RMSESD))","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:57:32.407992Z","iopub.execute_input":"2023-11-30T09:57:32.408524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-11-30T06:57:59.822443Z","iopub.execute_input":"2023-11-30T06:57:59.823235Z","iopub.status.idle":"2023-11-30T06:57:59.850221Z","shell.execute_reply.started":"2023-11-30T06:57:59.823202Z","shell.execute_reply":"2023-11-30T06:57:59.849352Z"},"trusted":true},"execution_count":null,"outputs":[]}]}